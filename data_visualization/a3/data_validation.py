"""
COVID-19 æ•°æ®è´¨é‡åˆ†æå’Œä¿®å¤è„šæœ¬
Data Quality Analysis and Repair Script for COVID-19 Dataset

åŠŸèƒ½ï¼š
1. æ•°æ®è´¨é‡åˆ†æ - è¯†åˆ«ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€ä¸ä¸€è‡´æ€§
2. æ•°æ®ä¿®å¤ - åŸºäºé€»è¾‘è§„åˆ™è¡¥å……ç¼ºå¤±æ•°æ®
3. æ•°æ®éªŒè¯ - ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
4. ç”Ÿæˆæ¸…æ´—åçš„å®Œæ•´æ•°æ®é›†
"""

import pandas as pd
import numpy as np
import warnings
from datetime import datetime, timedelta
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class COVID19DataValidator:
    """COVID-19æ•°æ®éªŒè¯å’Œä¿®å¤ç±»"""

    def __init__(self, csv_file_path):
        self.csv_file_path = csv_file_path
        self.df = None
        self.cleaned_df = None
        self.validation_report = {}

        # éå›½å®¶å®ä½“åˆ—è¡¨ï¼ˆéœ€è¦è¿‡æ»¤çš„èšåˆæ•°æ®ï¼‰
        self.non_country_entities = {
            "World",
            "Africa",
            "Asia",
            "Europe",
            "North America",
            "South America",
            "Oceania",
            "European Union",
            "High-income countries",
            "Low-income countries",
            "Lower-middle-income countries",
            "Upper-middle-income countries",
            "International",
        }

    def load_data(self):
        """åŠ è½½CSVæ•°æ®"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {self.csv_file_path}")
            self.df = pd.read_csv(self.csv_file_path)
            logger.info(f"æˆåŠŸåŠ è½½ {len(self.df)} è¡Œæ•°æ®")
            return True
        except Exception as e:
            logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return False

    def analyze_data_quality(self):
        """åˆ†ææ•°æ®è´¨é‡"""
        logger.info("å¼€å§‹æ•°æ®è´¨é‡åˆ†æ...")

        # åŸºæœ¬ä¿¡æ¯
        self.validation_report["basic_info"] = {
            "total_rows": len(self.df),
            "total_columns": len(self.df.columns),
            "date_range": (self.df["date"].min(), self.df["date"].max()),
            "unique_locations": self.df["location"].nunique(),
        }

        # å…³é”®å­—æ®µçš„ç¼ºå¤±å€¼åˆ†æ
        key_fields = ["new_cases", "new_deaths", "total_cases", "total_deaths"]
        missing_analysis = {}

        for field in key_fields:
            missing_count = self.df[field].isna().sum()
            empty_count = (self.df[field] == "").sum()
            zero_count = (self.df[field] == 0).sum()

            missing_analysis[field] = {
                "missing_values": missing_count,
                "empty_strings": empty_count,
                "zero_values": zero_count,
                "total_invalid": missing_count + empty_count,
                "valid_values": len(self.df) - missing_count - empty_count,
            }

        self.validation_report["missing_analysis"] = missing_analysis

        # æ•°æ®ç±»å‹åˆ†æ
        self.validation_report["data_types"] = self.df.dtypes.to_dict()

        # æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
        self._check_data_consistency()

        logger.info("æ•°æ®è´¨é‡åˆ†æå®Œæˆ")
        return self.validation_report

    def _check_data_consistency(self):
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        logger.info("æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§...")

        consistency_issues = []

        # 1. æ£€æŸ¥ç´¯è®¡æ•°æ®æ˜¯å¦å•è°ƒé€’å¢
        for location in self.df["location"].unique():
            location_data = self.df[self.df["location"] == location].sort_values("date")

            # æ£€æŸ¥total_caseså•è°ƒæ€§
            if not location_data["total_cases"].is_monotonic_increasing:
                non_monotonic = location_data[
                    location_data["total_cases"] < location_data["total_cases"].shift(1)
                ]
                if len(non_monotonic) > 0:
                    consistency_issues.append(
                        {
                            "type": "total_cases_decreasing",
                            "location": location,
                            "count": len(non_monotonic),
                            "dates": non_monotonic["date"].tolist(),
                        }
                    )

            # æ£€æŸ¥total_deathså•è°ƒæ€§
            if not location_data["total_deaths"].is_monotonic_increasing:
                non_monotonic = location_data[
                    location_data["total_deaths"]
                    < location_data["total_deaths"].shift(1)
                ]
                if len(non_monotonic) > 0:
                    consistency_issues.append(
                        {
                            "type": "total_deaths_decreasing",
                            "location": location,
                            "count": len(non_monotonic),
                            "dates": non_monotonic["date"].tolist(),
                        }
                    )

        self.validation_report["consistency_issues"] = consistency_issues

        # 2. æ£€æŸ¥æ–°å¢æ•°æ®ä¸ç´¯è®¡æ•°æ®çš„å…³ç³»
        self._check_new_vs_total_consistency()

    def _check_new_vs_total_consistency(self):
        """æ£€æŸ¥æ–°å¢æ•°æ®ä¸ç´¯è®¡æ•°æ®çš„ä¸€è‡´æ€§"""
        logger.info("æ£€æŸ¥æ–°å¢æ•°æ®ä¸ç´¯è®¡æ•°æ®çš„ä¸€è‡´æ€§...")

        new_total_issues = []

        for location in self.df["location"].unique():
            location_data = self.df[self.df["location"] == location].sort_values("date")

            # è®¡ç®—ç†è®ºä¸Šçš„æ–°å¢æ•°æ®
            location_data = location_data.copy()
            location_data["calculated_new_cases"] = location_data["total_cases"].diff()
            location_data["calculated_new_deaths"] = location_data[
                "total_deaths"
            ].diff()

            # æ£€æŸ¥new_casesä¸total_casesçš„å·®å¼‚
            new_cases_diff = location_data[
                (location_data["new_cases"].notna())
                & (location_data["calculated_new_cases"].notna())
                & (
                    abs(
                        location_data["new_cases"]
                        - location_data["calculated_new_cases"]
                    )
                    > 1
                )
            ]

            if len(new_cases_diff) > 0:
                new_total_issues.append(
                    {
                        "type": "new_cases_inconsistency",
                        "location": location,
                        "count": len(new_cases_diff),
                    }
                )

        self.validation_report["new_total_consistency_issues"] = new_total_issues

    def clean_and_repair_data(self):
        """æ¸…æ´—å’Œä¿®å¤æ•°æ®"""
        logger.info("å¼€å§‹æ•°æ®æ¸…æ´—å’Œä¿®å¤...")

        # å¤åˆ¶åŸå§‹æ•°æ®
        self.cleaned_df = self.df.copy()

        # 1. æ•°æ®ç±»å‹è½¬æ¢
        self._convert_data_types()

        # 2. å¤„ç†ç¼ºå¤±å€¼
        self._handle_missing_values()

        # 3. ä¿®å¤æ•°æ®ä¸€è‡´æ€§
        self._repair_consistency_issues()

        # 4. è¿‡æ»¤éå›½å®¶å®ä½“
        self._filter_non_country_entities()

        # 5. æœ€ç»ˆéªŒè¯
        self._final_validation()

        logger.info("æ•°æ®æ¸…æ´—å’Œä¿®å¤å®Œæˆ")
        return self.cleaned_df

    def _convert_data_types(self):
        """è½¬æ¢æ•°æ®ç±»å‹"""
        logger.info("è½¬æ¢æ•°æ®ç±»å‹...")

        numeric_fields = [
            "new_cases",
            "new_deaths",
            "total_cases",
            "total_deaths",
            "weekly_cases",
            "weekly_deaths",
            "biweekly_cases",
            "biweekly_deaths",
        ]

        for field in numeric_fields:
            # å°†ç©ºå­—ç¬¦ä¸²å’Œæ— æ•ˆå€¼è½¬æ¢ä¸ºNaN
            self.cleaned_df[field] = pd.to_numeric(
                self.cleaned_df[field], errors="coerce"
            )

        # è½¬æ¢æ—¥æœŸæ ¼å¼
        self.cleaned_df["date"] = pd.to_datetime(self.cleaned_df["date"])

        logger.info("æ•°æ®ç±»å‹è½¬æ¢å®Œæˆ")

    def _handle_missing_values(self):
        """å¤„ç†ç¼ºå¤±å€¼"""
        logger.info("å¤„ç†ç¼ºå¤±å€¼...")

        # æŒ‰å›½å®¶åˆ†ç»„å¤„ç†
        for location in self.cleaned_df["location"].unique():
            location_mask = self.cleaned_df["location"] == location
            location_data = self.cleaned_df[location_mask].sort_values("date")

            # å‰å‘å¡«å……ç´¯è®¡æ•°æ®
            self.cleaned_df.loc[location_mask, "total_cases"] = location_data[
                "total_cases"
            ].fillna(method="ffill")
            self.cleaned_df.loc[location_mask, "total_deaths"] = location_data[
                "total_deaths"
            ].fillna(method="ffill")

            # å¯¹äºæ–°å¢æ•°æ®ï¼Œå¦‚æœç¼ºå¤±åˆ™ç”¨0å¡«å……
            self.cleaned_df.loc[location_mask, "new_cases"] = location_data[
                "new_cases"
            ].fillna(0)
            self.cleaned_df.loc[location_mask, "new_deaths"] = location_data[
                "new_deaths"
            ].fillna(0)

        logger.info("ç¼ºå¤±å€¼å¤„ç†å®Œæˆ")

    def _repair_consistency_issues(self):
        """ä¿®å¤æ•°æ®ä¸€è‡´æ€§é—®é¢˜"""
        logger.info("ä¿®å¤æ•°æ®ä¸€è‡´æ€§é—®é¢˜...")

        for location in self.cleaned_df["location"].unique():
            location_mask = self.cleaned_df["location"] == location
            location_data = self.cleaned_df[location_mask].sort_values("date")

            # ç¡®ä¿ç´¯è®¡æ•°æ®å•è°ƒé€’å¢
            self.cleaned_df.loc[location_mask, "total_cases"] = location_data[
                "total_cases"
            ].cummax()
            self.cleaned_df.loc[location_mask, "total_deaths"] = location_data[
                "total_deaths"
            ].cummax()

            # é‡æ–°è®¡ç®—æ–°å¢æ•°æ®ä»¥ç¡®ä¿ä¸€è‡´æ€§
            location_data = self.cleaned_df[location_mask].sort_values("date")
            calculated_new_cases = (
                location_data["total_cases"].diff().fillna(location_data["total_cases"])
            )
            calculated_new_deaths = (
                location_data["total_deaths"]
                .diff()
                .fillna(location_data["total_deaths"])
            )

            # å¦‚æœåŸå§‹æ–°å¢æ•°æ®ä¸è®¡ç®—å€¼å·®å¼‚å¾ˆå¤§ï¼Œä½¿ç”¨è®¡ç®—å€¼
            mask_cases = abs(location_data["new_cases"] - calculated_new_cases) > 1
            mask_deaths = abs(location_data["new_deaths"] - calculated_new_deaths) > 1

            self.cleaned_df.loc[location_mask & mask_cases, "new_cases"] = (
                calculated_new_cases[mask_cases]
            )
            self.cleaned_df.loc[location_mask & mask_deaths, "new_deaths"] = (
                calculated_new_deaths[mask_deaths]
            )

        logger.info("æ•°æ®ä¸€è‡´æ€§ä¿®å¤å®Œæˆ")

    def _filter_non_country_entities(self):
        """è¿‡æ»¤éå›½å®¶å®ä½“"""
        logger.info("è¿‡æ»¤éå›½å®¶å®ä½“...")

        original_count = len(self.cleaned_df)
        self.cleaned_df = self.cleaned_df[
            ~self.cleaned_df["location"].isin(self.non_country_entities)
        ]
        filtered_count = len(self.cleaned_df)

        logger.info(f"è¿‡æ»¤äº† {original_count - filtered_count} è¡Œéå›½å®¶å®ä½“æ•°æ®")

    def _final_validation(self):
        """æœ€ç»ˆæ•°æ®éªŒè¯"""
        logger.info("è¿›è¡Œæœ€ç»ˆæ•°æ®éªŒè¯...")

        # ç¡®ä¿æ²¡æœ‰è´Ÿå€¼
        numeric_fields = ["new_cases", "new_deaths", "total_cases", "total_deaths"]
        for field in numeric_fields:
            negative_count = (self.cleaned_df[field] < 0).sum()
            if negative_count > 0:
                logger.warning(f"å‘ç° {negative_count} ä¸ªè´Ÿå€¼åœ¨å­—æ®µ {field}ï¼Œå°†è®¾ç½®ä¸º0")
                self.cleaned_df[field] = self.cleaned_df[field].clip(lower=0)

        # ç¡®ä¿ç´¯è®¡æ•°æ® >= æ–°å¢æ•°æ®
        for location in self.cleaned_df["location"].unique():
            location_mask = self.cleaned_df["location"] == location
            location_data = self.cleaned_df[location_mask].sort_values("date")

            # æ£€æŸ¥å¹¶ä¿®å¤ç´¯è®¡æ•°æ®å°äºæ–°å¢æ•°æ®çš„æƒ…å†µ
            mask = location_data["total_cases"] < location_data["new_cases"]
            if mask.any():
                logger.warning(f"ä¿®å¤ {location} çš„ç´¯è®¡ç—…ä¾‹æ•°æ®ä¸ä¸€è‡´é—®é¢˜")
                self.cleaned_df.loc[location_mask, "total_cases"] = location_data[
                    "total_cases"
                ].cumsum()

            mask = location_data["total_deaths"] < location_data["new_deaths"]
            if mask.any():
                logger.warning(f"ä¿®å¤ {location} çš„ç´¯è®¡æ­»äº¡æ•°æ®ä¸ä¸€è‡´é—®é¢˜")
                self.cleaned_df.loc[location_mask, "total_deaths"] = location_data[
                    "total_deaths"
                ].cumsum()

        logger.info("æœ€ç»ˆæ•°æ®éªŒè¯å®Œæˆ")

    def generate_cleaned_dataset(self, output_file=None):
        """ç”Ÿæˆæ¸…æ´—åçš„æ•°æ®é›†"""
        if output_file is None:
            output_file = self.csv_file_path.replace(".csv", "_cleaned.csv")

        logger.info(f"ç”Ÿæˆæ¸…æ´—åçš„æ•°æ®é›†: {output_file}")

        # ç¡®ä¿æ—¥æœŸæ ¼å¼æ­£ç¡®
        self.cleaned_df["date"] = self.cleaned_df["date"].dt.strftime("%Y-%m-%d")

        # ä¿å­˜åˆ°CSV
        self.cleaned_df.to_csv(output_file, index=False)

        logger.info(f"æ¸…æ´—åçš„æ•°æ®é›†å·²ä¿å­˜: {output_file}")
        return output_file

    def generate_validation_report(self, output_file=None):
        """ç”Ÿæˆæ•°æ®éªŒè¯æŠ¥å‘Š"""
        if output_file is None:
            output_file = "data_validation_report.txt"

        logger.info(f"ç”Ÿæˆæ•°æ®éªŒè¯æŠ¥å‘Š: {output_file}")

        with open(output_file, "w", encoding="utf-8") as f:
            f.write("COVID-19 æ•°æ®è´¨é‡éªŒè¯æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            # åŸºæœ¬ä¿¡æ¯
            f.write("1. åŸºæœ¬ä¿¡æ¯\n")
            f.write("-" * 20 + "\n")
            basic_info = self.validation_report["basic_info"]
            f.write(f"æ€»è¡Œæ•°: {basic_info['total_rows']:,}\n")
            f.write(f"æ€»åˆ—æ•°: {basic_info['total_columns']}\n")
            f.write(
                f"æ—¥æœŸèŒƒå›´: {basic_info['date_range'][0]} åˆ° {basic_info['date_range'][1]}\n"
            )
            f.write(f"ä¸åŒåœ°åŒºæ•°: {basic_info['unique_locations']:,}\n\n")

            # ç¼ºå¤±å€¼åˆ†æ
            f.write("2. ç¼ºå¤±å€¼åˆ†æ\n")
            f.write("-" * 20 + "\n")
            for field, info in self.validation_report["missing_analysis"].items():
                f.write(f"{field}:\n")
                f.write(f"  ç¼ºå¤±å€¼: {info['missing_values']:,}\n")
                f.write(f"  ç©ºå­—ç¬¦ä¸²: {info['empty_strings']:,}\n")
                f.write(f"  é›¶å€¼: {info['zero_values']:,}\n")
                f.write(f"  æœ‰æ•ˆå€¼: {info['valid_values']:,}\n\n")

            # ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
            f.write("3. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥\n")
            f.write("-" * 20 + "\n")
            consistency_issues = self.validation_report.get("consistency_issues", [])
            f.write(f"å‘ç° {len(consistency_issues)} ä¸ªä¸€è‡´æ€§é—®é¢˜\n")

            for issue in consistency_issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                f.write(
                    f"- {issue['type']} in {issue['location']}: {issue['count']} ä¸ªé—®é¢˜\n"
                )

            if len(consistency_issues) > 10:
                f.write(f"... è¿˜æœ‰ {len(consistency_issues) - 10} ä¸ªé—®é¢˜\n")

            f.write("\n")

            # æ¸…æ´—åæ•°æ®ç»Ÿè®¡
            if self.cleaned_df is not None:
                f.write("4. æ¸…æ´—åæ•°æ®ç»Ÿè®¡\n")
                f.write("-" * 20 + "\n")
                f.write(f"æ¸…æ´—åè¡Œæ•°: {len(self.cleaned_df):,}\n")
                f.write(f"æ¸…æ´—ååœ°åŒºæ•°: {self.cleaned_df['location'].nunique():,}\n")
                f.write(f"æœ€æ–°æ—¥æœŸ: {self.cleaned_df['date'].max()}\n")

                # æœ€æ–°æ—¥æœŸçš„æ•°æ®ç»Ÿè®¡
                latest_date = self.cleaned_df["date"].max()
                latest_data = self.cleaned_df[self.cleaned_df["date"] == latest_date]
                f.write(f"æœ€æ–°æ—¥æœŸæ•°æ®ç‚¹æ•°: {len(latest_data):,}\n")

                # æœ‰æ•ˆæ•°æ®ç»Ÿè®¡
                valid_total = latest_data[
                    (latest_data["total_cases"] > 0) & (latest_data["total_deaths"] > 0)
                ]
                valid_new = latest_data[
                    (latest_data["new_cases"] > 0) | (latest_data["new_deaths"] > 0)
                ]
                f.write(f"æœ‰æ•ˆç´¯è®¡æ•°æ®ç‚¹æ•°: {len(valid_total):,}\n")
                f.write(f"æœ‰æ•ˆæ–°å¢æ•°æ®ç‚¹æ•°: {len(valid_new):,}\n")

        logger.info(f"æ•°æ®éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        return output_file


def main():
    """ä¸»å‡½æ•°"""
    print("COVID-19 æ•°æ®è´¨é‡åˆ†æå’Œä¿®å¤å·¥å…·")
    print("=" * 50)

    # åˆå§‹åŒ–éªŒè¯å™¨
    validator = COVID19DataValidator("covid-19_full_data.csv")

    # åŠ è½½æ•°æ®
    if not validator.load_data():
        return

    # åˆ†ææ•°æ®è´¨é‡
    print("\n1. åˆ†ææ•°æ®è´¨é‡...")
    validation_report = validator.analyze_data_quality()

    # æ¸…æ´—å’Œä¿®å¤æ•°æ®
    print("\n2. æ¸…æ´—å’Œä¿®å¤æ•°æ®...")
    cleaned_df = validator.clean_and_repair_data()

    # ç”Ÿæˆæ¸…æ´—åçš„æ•°æ®é›†
    print("\n3. ç”Ÿæˆæ¸…æ´—åçš„æ•°æ®é›†...")
    cleaned_file = validator.generate_cleaned_dataset()

    # ç”ŸæˆéªŒè¯æŠ¥å‘Š
    print("\n4. ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
    report_file = validator.generate_validation_report()

    print(f"\nâœ… æ•°æ®æ¸…æ´—å®Œæˆï¼")
    print(f"ğŸ“ æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶: {cleaned_file}")
    print(f"ğŸ“Š éªŒè¯æŠ¥å‘Š: {report_file}")
    print(f"ğŸ“ˆ åŸå§‹æ•°æ®è¡Œæ•°: {len(validator.df):,}")
    print(f"ğŸ“ˆ æ¸…æ´—åæ•°æ®è¡Œæ•°: {len(cleaned_df):,}")


if __name__ == "__main__":
    main()
